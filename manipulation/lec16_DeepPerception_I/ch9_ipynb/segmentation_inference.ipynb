{"cells":[{"cell_type":"markdown","metadata":{"id":"DfPPQ6ztJhv4","cell_id":"10fa63c82c224139a9c8b047fa9c236f","deepnote_cell_type":"markdown"},"source":"# Mask R-CNN for Bin Picking\n\nThis notebook is adopted from the [TorchVision 0.3 Object Detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).  We will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on a dataset generated from our \"clutter generator\" script.\n","block_group":"56c391cc4faf4fa4af0f8009abaf948b"},{"cell_type":"code","metadata":{"id":"DBIoe_tHTQgV","cell_id":"f8b28a88ddf14fefbcaeb11097bac60d","deepnote_cell_type":"code"},"source":"# Imports\nimport fnmatch\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nycb = [\n    \"003_cracker_box.sdf\",\n    \"004_sugar_box.sdf\",\n    \"005_tomato_soup_can.sdf\",\n    \"006_mustard_bottle.sdf\",\n    \"009_gelatin_box.sdf\",\n    \"010_potted_meat_can.sdf\",\n]","block_group":"b8d9268ffff74280b60545ecff20cfb9","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"XwyE5A8DGtct","cell_id":"6d5c67d1b3a64462b3b5f864b2254dbf","deepnote_cell_type":"markdown"},"source":"# Download our bin-picking model\n\nAnd a small set of images for testing.","block_group":"4270ea3e3e084809ba5bdf34c385143a"},{"cell_type":"code","metadata":{"id":"_DgAgqauIET9","cell_id":"85a3be4389594c2b950b6551af3d675b","deepnote_cell_type":"code"},"source":"dataset_path = \"clutter_maskrcnn_data\"\nif not os.path.exists(dataset_path):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_test.zip .\n    !unzip -q clutter_maskrcnn_test.zip\n\nnum_images = len(fnmatch.filter(os.listdir(dataset_path), \"*.png\"))\n\n\ndef open_image(idx):\n    filename = os.path.join(dataset_path, f\"{idx:05d}.png\")\n    return Image.open(filename).convert(\"RGB\")\n\n\nmodel_file = \"clutter_maskrcnn_model.pt\"\nif not os.path.exists(model_file):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_model.pt .","block_group":"83559000104e45278e424a81b841089a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"xA8sBvuHNNH1","cell_id":"60d323d667734378a744928cb8228420","deepnote_cell_type":"markdown"},"source":"# Load the model","block_group":"32636dee920f42d18733f2450b1e7341"},{"cell_type":"code","metadata":{"id":"vUJXn15pGzRj","cell_id":"c777b796c4894619934778ef37f56034","deepnote_cell_type":"code"},"source":"import torchvision\nimport torchvision.transforms.functional as Tf\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    )\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    return model\n\n\nnum_classes = len(ycb) + 1\nmodel = get_instance_segmentation_model(num_classes)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.load_state_dict(torch.load(\"clutter_maskrcnn_model.pt\", map_location=device))\nmodel.eval()\n\nmodel.to(device)","block_group":"22a8d41aaefa466ea82fb82b02e76abe","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"Z6mYGFLxkO8F","cell_id":"222b11715aab46cd887514c1b32cc6d5","deepnote_cell_type":"markdown"},"source":"# Evaluate the network","block_group":"27544ecda40c41f192829a9a818a9775"},{"cell_type":"code","metadata":{"id":"YHwIdxH76uPj","cell_id":"8c9e1d84e3574c0d843ebe805b78699d","deepnote_cell_type":"code"},"source":"# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9952)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])","block_group":"912dc68505bb494f930ac401af126072","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"DmN602iKsuey","cell_id":"a0eab47432374907baad9e16d12316a2","deepnote_cell_type":"markdown"},"source":"Printing the prediction shows that we have a list of dictionaries. Each element\nof the list corresponds to a different image; since we have a single image,\nthere is a single dictionary in the list. The dictionary contains the\npredictions for the image we passed. In this case, we can see that it contains\n`boxes`, `labels`, `masks` and `scores` as fields.","block_group":"2fe4eb782b304c44849b30b7225eafff"},{"cell_type":"code","metadata":{"id":"Lkmb3qUu6zw3","cell_id":"7626024789894705a9f6f58caf6db2f8","deepnote_cell_type":"code"},"source":"prediction","block_group":"df93af41253e4b8eb096a5073380b8c9","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"RwT21rzotFbH","cell_id":"7f1d820b01f14cf48a58ba9353749044","deepnote_cell_type":"markdown"},"source":"Let's inspect the image and the predicted segmentation masks.\n\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.","block_group":"66b75278aa744df995940fb04693fbf6"},{"cell_type":"code","metadata":{"id":"bpqN9t1u7B2J","cell_id":"de756b6632f249f6ad2e27689ca162df","deepnote_cell_type":"code"},"source":"img","block_group":"c9d6435d1def47159fc0b69599accec1","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"M58J3O9OtT1G","cell_id":"f7c67746aa7b45dab3b00af3ddfa4e6b","deepnote_cell_type":"markdown"},"source":"And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1.","block_group":"63b6c1f0ea624bc1a978df61c4f21411"},{"cell_type":"code","metadata":{"id":"5v5S3bm07SO1","cell_id":"dfbd04e33c2b4d19adad470b5df15ca1","deepnote_cell_type":"code"},"source":"N = prediction[0][\"masks\"].shape[0]\nfig, ax = plt.subplots(N, 1, figsize=(15, 15))\nfor n in range(prediction[0][\"masks\"].shape[0]):\n    ax[n].imshow(\n        np.asarray(\n            Image.fromarray(prediction[0][\"masks\"][n, 0].mul(255).byte().cpu().numpy())\n        )\n    )","block_group":"a83327a20fe64641880063630b8cbbdd","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"z9QAeX9HkDTx","cell_id":"139a91fff8614c25b7197c984b9a4bbf","deepnote_cell_type":"markdown"},"source":"# Plot the object detections","block_group":"379994a21d8d43ce96aaf71e1af68c37"},{"cell_type":"code","metadata":{"id":"Z08keVFkvtPh","cell_id":"6c0b6520d6704019a2cc2625e9cef2a3","deepnote_cell_type":"code"},"source":"import random\n\nimport matplotlib.patches as patches\n\n\ndef plot_prediction():\n    img_np = np.array(img)\n    fig, ax = plt.subplots(1, figsize=(12, 9))\n    ax.imshow(img_np)\n\n    cmap = plt.get_cmap(\"tab20b\")\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    num_instances = prediction[0][\"boxes\"].shape[0]\n    bbox_colors = random.sample(colors, num_instances)\n    boxes = prediction[0][\"boxes\"].cpu().numpy()\n    labels = prediction[0][\"labels\"].cpu().numpy()\n\n    for i in range(num_instances):\n        color = bbox_colors[i]\n        bb = boxes[i, :]\n        bbox = patches.Rectangle(\n            (bb[0], bb[1]),\n            bb[2] - bb[0],\n            bb[3] - bb[1],\n            linewidth=2,\n            edgecolor=color,\n            facecolor=\"none\",\n        )\n        ax.add_patch(bbox)\n        plt.text(\n            bb[0],\n            bb[0],\n            s=ycb[labels[i]],\n            color=\"white\",\n            verticalalignment=\"top\",\n            bbox={\"color\": color, \"pad\": 0},\n        )\n    plt.axis(\"off\")\n\n\nplot_prediction()","block_group":"3937e1fa5ba34bacb61a7f7ea7e1030e","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"HIfmykN-t7XG","cell_id":"4fc7e7555f784ee7a8eb4aafda177147","deepnote_cell_type":"markdown"},"source":"# Visualize the region proposals \n\nLet's visualize some of the intermediate results of the networks.\n\nTODO: would be very cool to put a slider on this so that we could slide through ALL of the boxes.  But my matplotlib non-interactive backend makes it too tricky!","block_group":"5e272858162b44cfa12d9fff9dc14988"},{"cell_type":"code","metadata":{"id":"zBNqFb68td8N","cell_id":"88302903f84a494d9a41344a81cf63cc","deepnote_cell_type":"code"},"source":"class Inspector:\n    \"\"\"A helper class from Kuni to be used for torch.nn.Module.register_forward_hook.\"\"\"\n\n    def __init__(self):\n        self.x = None\n\n    def hook(self, module, input, output):\n        self.x = output\n\n\ninspector = Inspector()\nmodel.rpn.register_forward_hook(inspector.hook)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nrpn_values = inspector.x\n\n\nimg_np = np.array(img)\nplt.figure()\nfig, ax = plt.subplots(1, figsize=(12, 9))\nax.imshow(img_np)\n\ncmap = plt.get_cmap(\"tab20b\")\ncolors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\nnum_to_draw = 20\nbbox_colors = random.sample(colors, num_to_draw)\nboxes = rpn_values[0][0].cpu().numpy()\nprint(f\"Region proposals (drawing first {num_to_draw} out of {boxes.shape[0]})\")\n\nfor i in range(num_to_draw):\n    color = bbox_colors[i]\n    bb = boxes[i, :]\n    bbox = patches.Rectangle(\n        (bb[0], bb[1]),\n        bb[2] - bb[0],\n        bb[3] - bb[1],\n        linewidth=2,\n        edgecolor=color,\n        facecolor=\"none\",\n    )\n    ax.add_patch(bbox)\nplt.axis(\"off\");","block_group":"b0d1f5c649594645b490ff3d4c1ca44a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"ce6b3f148af24e26be7c24f8fe967c09","deepnote_cell_type":"markdown"},"source":"# Try a few more images","block_group":"aa2b0033af1f4ff29a0f231195548243"},{"cell_type":"code","metadata":{"cell_id":"7cf5843bfbe440bb97855d887836bf86","deepnote_cell_type":"code"},"source":"# pick one image from the test set (choose between 9950 and 9999)\nimg = open_image(9985)\n\nwith torch.no_grad():\n    prediction = model([Tf.to_tensor(img).to(device)])\n\nplot_prediction()","block_group":"867df3f4e8ad45129dac0dfe72ad0ec9","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"aa299bf0301c45b9a135e77402e6a9b6","deepnote_cell_type":"code"},"source":"","block_group":"07c98ae48bd74044bff1c3ca562abbc4","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=deee6dca-8c5f-4baa-bc36-a4211e6ce8ad' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"b585a0d307194d66ab4a1d87a1ad3625","deepnote_execution_queue":[]}}