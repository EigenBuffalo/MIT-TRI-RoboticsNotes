{"cells":[{"cell_type":"markdown","metadata":{"id":"DfPPQ6ztJhv4","cell_id":"938619b5e2674cb09dc94a09ceabc5a3","deepnote_cell_type":"markdown"},"source":"# Mask R-CNN for Bin Picking\n\nThis notebook is adopted from the [TorchVision 0.3 Object Detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).  We will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on a dataset generated from our \"clutter generator\" script.\n","block_group":"cb788e9906504d599c0cc40b0917bc1b"},{"cell_type":"code","metadata":{"id":"DBIoe_tHTQgV","cell_id":"8612aa64b240462fbd84b59c911bb3ac","deepnote_cell_type":"code"},"source":"!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n\n# Download TorchVision repo to use some files from\n# references/detection\n!git clone https://github.com/pytorch/vision.git\n!cd vision && git checkout v0.3.0\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./\n\n# Imports\nimport fnmatch\nimport json\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nycb = [\n    \"003_cracker_box.sdf\",\n    \"004_sugar_box.sdf\",\n    \"005_tomato_soup_can.sdf\",\n    \"006_mustard_bottle.sdf\",\n    \"009_gelatin_box.sdf\",\n    \"010_potted_meat_can.sdf\",\n]\n\n# drake_reserved_labels = [32765, 32764, 32766, 32767]\n\n\ndef colorize_labels(image):\n    \"\"\"Colorizes labels.\"\"\"\n    cc = mpl.colors.ColorConverter()\n    color_cycle = plt.rcParams[\"axes.prop_cycle\"]\n    colors = np.array([cc.to_rgb(c[\"color\"]) for c in color_cycle])\n    bg_color = [0, 0, 0]\n    image = np.squeeze(image)\n    background = np.zeros(image.shape[:2], dtype=bool)\n    for label in reserved_labels:\n        background |= image == int(label)\n    image[np.logical_not(background)]\n    color_image = colors[image % len(colors)]\n    color_image[background] = bg_color\n    return color_image","block_group":"36c6e79e5799419a9aca2eed1fee89b9","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"XwyE5A8DGtct","cell_id":"17502b0dbbe941ed9a8e7dcfc83a3cf5","deepnote_cell_type":"markdown"},"source":"# Download our bin-picking dataset\n\nIt's definitely possible to actually create this dataset on Colab; I've just written a version of the \"clutter_gen\" method from the last chapter that writes the images (and label images) to disk, along with some annotations.  But it takes a non-trivial amount of time to generate 10,000 images. \n","block_group":"45079fc46f6548cbafc8abea046e4091"},{"cell_type":"code","metadata":{"id":"_DgAgqauIET9","cell_id":"9d43eabffee04b13ad6b12a2d3cea3fd","deepnote_cell_type":"code"},"source":"dataset_path = \"clutter_maskrcnn_data\"\nif not os.path.exists(dataset_path):\n    !wget https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_data.zip .\n    !unzip -q clutter_maskrcnn_data.zip","block_group":"4f7d108898144919a83920738cbf1548","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"xA8sBvuHNNH1","cell_id":"f82fdd40b15c4adfa51db4ebe927ab39","deepnote_cell_type":"markdown"},"source":"If you are on colab, go ahead and use the file browser on the left (looks like a drive under the table of contents panel) to click through the .png and .json files to make sure you understand the dataset you've just created!  If you're on a local machine, just browse to the folder.","block_group":"fffea70d8f4744be81eec9208ea18a9c"},{"cell_type":"markdown","metadata":{"id":"C9Ee5NV54Dmj","cell_id":"e44f2b240e514643b49818701c1100dc","deepnote_cell_type":"markdown"},"source":"# Teach pytorch how to load the dataset\n\ninto the [format expected by Mask R-CNN](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.detection.maskrcnn_resnet50_fpn).","block_group":"6462e2e871384a90a6b8cdeba98cc6b1"},{"cell_type":"code","metadata":{"id":"mTgWtixZTs3X","cell_id":"c892bd16bbed4e9c8ba8310ae0700632","deepnote_cell_type":"code"},"source":"class BinPickingDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.num_images = len(fnmatch.filter(os.listdir(root), \"*.png\"))\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        filename_base = os.path.join(self.root, f\"{idx:05d}\")\n\n        img = Image.open(filename_base + \".png\").convert(\"RGB\")\n        mask = np.squeeze(np.load(filename_base + \"_mask.npy\"))\n\n        with open(filename_base + \".json\", \"r\") as f:\n            instance_id_to_class_name = json.load(f)\n        labels = ycb == instance_id_to_class_name\n\n        # instances are encoded as different colors\n        obj_ids = np.asarray(list(instance_id_to_class_name.keys()))\n        count = (mask == np.int16(obj_ids)[:, None, None]).sum(axis=2).sum(axis=1)\n\n        # discard objects instances with less than 10 pixels\n        obj_ids = obj_ids[count >= 10]\n\n        labels = [ycb.index(instance_id_to_class_name[id] + \".sdf\") for id in obj_ids]\n        obj_ids = np.int16(np.asarray(obj_ids))\n\n        # split the color-encoded mask into a set of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return self.num_images","block_group":"ccf85e837f1d4ebc9492443c9239f270","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"J6f3ZOTJ4Km9","cell_id":"89153b883b1c442f83ff8fba8d8c48ca","deepnote_cell_type":"markdown"},"source":"Let's check the output of our dataset.","block_group":"8fdba644120d48a08c3616404a70213e"},{"cell_type":"code","metadata":{"id":"ZEARO4B_ye0s","cell_id":"45a0b0f776014625bae9b6f8f78841ff","deepnote_cell_type":"code"},"source":"dataset = BinPickingDataset(dataset_path)\ndataset[0][0]","block_group":"6cfcf6a02b714afba8b08e4da66f61cf","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"xWA2NXwVhV_C","cell_id":"fb726bf0094a49f6ae1f984820b760aa","deepnote_cell_type":"markdown"},"source":"# Define the network\n\nThis cell is where the magic begins to happen.  We load a network that is pre-trained on the COCO dataset, then replace the network head with a new (untrained) network with the right number of outputs for our YCB recognition/segmentation task.","block_group":"7134ec31379d458d9fa9edc651d5a512"},{"cell_type":"code","metadata":{"id":"YjNHjVMOyYlH","cell_id":"87d5a123b5464a3cab242a8dec6cd825","deepnote_cell_type":"code"},"source":"import torchvision\nfrom torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n    )\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    return model","block_group":"67825ac70a154c1093f906f8d6475cfe","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"-WXLwePV5ieP","cell_id":"13d1ac8e9686495dadcd856c280649cf","deepnote_cell_type":"markdown"},"source":"That's it, this will make model be ready to be trained and evaluated on our custom dataset.\n\n# Transforms\n\nLet's write some helper functions for data augmentation / transformation, which leverages the functions in torchvision `refereces/detection`. \n","block_group":"a2243994d64c4628bae37197bf4740d9"},{"cell_type":"code","metadata":{"id":"l79ivkwKy357","cell_id":"d581bb2ef68e4ee9a405fd1866a592f3","deepnote_cell_type":"code"},"source":"import transforms as T\nimport utils\nfrom engine import evaluate, train_one_epoch\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","block_group":"852c541564654edbafb676905b3826d0","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"FzCLqiZk-sjf","cell_id":"18c5fa2337974bb7afc08d659bdebdb3","deepnote_cell_type":"markdown"},"source":"Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.","block_group":"36c7c02ad1d142028173671a1dedd59f"},{"cell_type":"markdown","metadata":{"id":"3YFJGJxk6XEs","cell_id":"58c1a134f52941f0870003bc2bac1d47","deepnote_cell_type":"markdown"},"source":"# Putting everything together\n\nWe now have the dataset class, the models and the data transforms. Let's instantiate them","block_group":"489b1fba3bc84fd48a1f6d8b42e28da3"},{"cell_type":"code","metadata":{"id":"a5dGaIezze3y","cell_id":"7ed1bf617f9f4c6d996b29798aa8712f","deepnote_cell_type":"code"},"source":"# use our dataset and defined transformations\ndataset = BinPickingDataset(dataset_path, get_transform(train=True))\ndataset_test = BinPickingDataset(dataset_path, get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=utils.collate_fn,\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=utils.collate_fn,\n)","block_group":"f9f3da51fe494a09a217564be9ab5ece","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"L5yvZUprj4ZN","cell_id":"95cab79f8e744094b111e3986386dc5a","deepnote_cell_type":"markdown"},"source":"Now let's instantiate the model and the optimizer","block_group":"7ef58af59f73402ca3a163cc8ea80662"},{"cell_type":"code","metadata":{"id":"zoenkCj18C4h","cell_id":"e8a52b1e23f546fda034aa1a30b3657a","deepnote_cell_type":"code"},"source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nnum_classes = len(ycb) + 1\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","block_group":"a7f2abe0ca5b4c469ce7721bead2a407","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"XAd56lt4kDxc","cell_id":"e2a87a5338b84dbeb8030e6911074552","deepnote_cell_type":"markdown"},"source":"And now let's train the model for 10 epochs, evaluating at the end of every epoch.","block_group":"d7c18f44bf9f4652ac6d8705830a18da"},{"cell_type":"code","metadata":{"id":"at-h4OWK0aoc","cell_id":"8abea321ed3244c09aa512fdf9a217c3","deepnote_cell_type":"code"},"source":"# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)","block_group":"69ef869d54d740d0a77abd2fa299086d","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"XXTyZhCScUTI","cell_id":"9d636985e3964fff8714ffd0f58dc1fe","deepnote_cell_type":"markdown"},"source":"If you're going to leave this running for a bit, I recommend scheduling the following cell to run immediately (so that you don't lose your work).","block_group":"947d11b074474f4eb32f419ec37f646b"},{"cell_type":"code","metadata":{"id":"vUJXn15pGzRj","cell_id":"71f31f590d0a4408a2e234555ea5e4d0","deepnote_cell_type":"code"},"source":"torch.save(model.state_dict(), \"clutter_maskrcnn_model.pt\")\n\nfrom google.colab import files\n\nfiles.download(\"clutter_maskrcnn_model.pt\")","block_group":"1405d53487cb41c8a222eec05c6a421b","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"Z6mYGFLxkO8F","cell_id":"602e910e695a45dcbfdbbf7ef00f34df","deepnote_cell_type":"markdown"},"source":"Now that training has finished, let's have a look at what it actually predicts in a test image","block_group":"ba7a266820fd4dae9a9a553fd1cd703e"},{"cell_type":"code","metadata":{"id":"YHwIdxH76uPj","cell_id":"db7b18d8e71f4478b0ce9354ba6651d2","deepnote_cell_type":"code"},"source":"# pick one image from the test set\nimg, _ = dataset_test[0]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])","block_group":"32a23081d8c84c9e9433ae22174f5e2b","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"DmN602iKsuey","cell_id":"344bbad6d0a64368b4bf7db82530a9fa","deepnote_cell_type":"markdown"},"source":"Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\nThe dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields.","block_group":"345e33c97df04152840001edf1783eb1"},{"cell_type":"code","metadata":{"id":"Lkmb3qUu6zw3","cell_id":"c14512ca770c486dad743b128dd13bb6","deepnote_cell_type":"code"},"source":"prediction","block_group":"aafbb17d55ba4bc4a7d27ddb124d9270","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"RwT21rzotFbH","cell_id":"ac6462120c5e41bd8a6bcaf5bd445694","deepnote_cell_type":"markdown"},"source":"Let's inspect the image and the predicted segmentation masks.\n\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.","block_group":"e84b74d83fa74699b21737576fbab4cb"},{"cell_type":"code","metadata":{"id":"bpqN9t1u7B2J","cell_id":"e36d4bbfa04a40108a83fe574d30f918","deepnote_cell_type":"code"},"source":"Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())","block_group":"b8710e36b531419ca78da9786966306d","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"M58J3O9OtT1G","cell_id":"fbb67846689d4fdb874f21a684f5074e","deepnote_cell_type":"markdown"},"source":"And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1.","block_group":"20f2740ffbbf4ba18b4a093d9e89ef5a"},{"cell_type":"code","metadata":{"id":"5v5S3bm07SO1","cell_id":"bf7963adbcf74096854138100d023695","deepnote_cell_type":"code"},"source":"Image.fromarray(prediction[0][\"masks\"][0, 0].mul(255).byte().cpu().numpy())","block_group":"ca1b2a0c116f4cbfa5db70a085cb9f48","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=deee6dca-8c5f-4baa-bc36-a4211e6ce8ad' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"a67c1b93d03c42dda577afc307d59b97","deepnote_execution_queue":[]}}