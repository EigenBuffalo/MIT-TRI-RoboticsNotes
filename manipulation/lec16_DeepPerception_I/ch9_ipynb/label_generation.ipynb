{"cells":[{"cell_type":"markdown","metadata":{"id":"gewjvw0krZOX","cell_id":"c8f69ab8e4d64a199a766a1eca9ecca5","deepnote_cell_type":"markdown"},"source":"# Mask-RCNN Label Generation","block_group":"b72533d3dfe44341b40e5c7ff13e3aff"},{"cell_type":"code","metadata":{"id":"4budwoxsrZOY","cell_id":"6608c7b108df4c418ee85866c3210d36","deepnote_cell_type":"code"},"source":"from copy import deepcopy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pydrake.all import RigidTransform, StartMeshcat\n\nfrom manipulation.mustard_depth_camera_example import MustardExampleSystem","block_group":"b3663e439365408fbd2b73afb31c48c9","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"6b4caa3cf0e146ef8ddbf405cd36de1b","deepnote_cell_type":"code"},"source":"# Start the visualizer.\nmeshcat = StartMeshcat()","block_group":"b34082dca6c54dcb8c57290970a8a54e","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"Hpzje8Y0rZOd","outputId":"41c123c5-3303-4dda-c7ca-0e1d997c9e2c","cell_id":"8b44886f71414f03a8080c6ee47090de","deepnote_cell_type":"code"},"source":"class SimpleCameraSystem:\n    def __init__(self):\n        diagram = MustardExampleSystem()\n        context = diagram.CreateDefaultContext()\n\n        # setup\n        meshcat.SetProperty(\"/Background\", \"visible\", False)\n\n        # getting data\n        self.point_cloud = diagram.GetOutputPort(\"camera0_point_cloud\").Eval(context)\n        self.depth_im_read = (\n            diagram.GetOutputPort(\"camera0_depth_image\").Eval(context).data.squeeze()\n        )\n        self.depth_im = deepcopy(self.depth_im_read)\n        self.depth_im[self.depth_im == np.inf] = 10.0\n        label_im = (\n            diagram.GetOutputPort(\"camera0_label_image\").Eval(context).data.squeeze()\n        )\n        self.rgb_im = diagram.GetOutputPort(\"camera0_rgb_image\").Eval(context).data\n        self.mask = label_im == 1\n\n        # draw visualization\n        meshcat.SetObject(\"point_cloud\", self.point_cloud)\n\n        # camera specs\n        cam0 = diagram.GetSubsystemByName(\"camera0\")\n        cam0_context = cam0.GetMyMutableContextFromRoot(context)\n        self.X_WC = cam0.GetOutputPort(\"body_pose_in_world\").Eval(cam0_context)\n        self.X_WC = RigidTransform(self.X_WC)  # See drake issue #15973\n        self.cam_info = cam0.depth_camera_info()\n\n        # get points for mustard bottle\n        depth_mustard = self.mask * self.depth_im\n        u_range = np.arange(depth_mustard.shape[0])\n        v_range = np.arange(depth_mustard.shape[1])\n        depth_v, depth_u = np.meshgrid(v_range, u_range)\n        depth_pnts = np.dstack([depth_u, depth_v, depth_mustard])\n        depth_pnts = depth_pnts.reshape([depth_pnts.shape[0] * depth_pnts.shape[1], 3])\n        pC = self.project_depth_to_pC(depth_pnts)\n        p_C_mustard = pC[pC[:, 2] > 0]\n        self.p_W_mustard = self.X_WC.multiply(p_C_mustard.T).T\n\n    def get_color_image(self):\n        return deepcopy(self.rgb_im[:, :, 0:3])\n\n    def get_intrinsics(self):\n        # read camera intrinsics\n        cx = self.cam_info.center_x()\n        cy = self.cam_info.center_y()\n        fx = self.cam_info.focal_x()\n        fy = self.cam_info.focal_y()\n        return cx, cy, fx, fy\n\n    def project_depth_to_pC(self, depth_pixel):\n        \"\"\"\n        project depth pixels to points in camera frame\n        using pinhole camera model\n        Input:\n            depth_pixels: numpy array of (nx3) or (3,)\n        Output:\n            pC: 3D point in camera frame, numpy array of (nx3)\n        \"\"\"\n        # switch u,v due to python convention\n        v = depth_pixel[:, 0]\n        u = depth_pixel[:, 1]\n        Z = depth_pixel[:, 2]\n        cx, cy, fx, fy = self.get_intrinsics()\n        X = (u - cx) * Z / fx\n        Y = (v - cy) * Z / fy\n        pC = np.c_[X, Y, Z]\n        return pC\n\n\ndef bbox(img):\n    a = np.where(img != 0)\n    bbox = ([np.min(a[0]), np.max(a[0])], [np.min(a[1]), np.max(a[1])])\n    return bbox\n\n\nenv = SimpleCameraSystem()\nX_WC = env.X_WC\np_W_mustard = env.p_W_mustard\nK = env.cam_info.intrinsic_matrix()\nrgb_im = env.get_color_image()","block_group":"9efea26c4b2f41d1acf6127e40efbb2d","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"2fj0D-mHrZOn","cell_id":"6da8e46e16c247039a29eed7940b0cd5","deepnote_cell_type":"markdown"},"source":"# Generate Mask Labels\n\nIn the lecture, you have learned about Mask-RCNN. A major difficulty in training/fine-tuning Mask-RCNN is to obtain high-quality real training data, especially the mask labels for the objects of interest. Although you can get training labels from [Amazon Mechanical Turk](https://www.mturk.com/), it is a paid service and you will have to wait for some time until you get your data labeled. An alternative method is to design clever pipelines to generate labeled masks automatically without requiring manual labor.  \n\nConsider a setup where an object of interest is placed on a planar surface, and an RGBD camera is mounted at a fixed location pointing to the object. From the RGBD camera, you should be able to generate the corresponding point clouds of the desired object and the surrounding environment (e.g. planar surface). You can easily remove the points associated with the planar surface (recall RANSAC exercise in the problem set 2). The remaining points then should all belong to the desired object. To generate mask labels, all you need to do is to project the points back to the camera image plane using the pinhole camera model!\n\nLet's quickly review the pinhole camera model!\n\nIn problem set 5, you played with [pinhole camera model](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html). In particular, you used the pinhole camera model to map the depth pixels to 3D points. See the `SimpleCameraSystem` class above to review how this works.\n\nThe mathematical description of the pinhole camera model is written below (you may also use the intrinsics matrix by `env.cam_info.intrinsic_matrix()`).\n\nThe camera intrinsics are:\n$$X_c = (u-c_x)\\frac{Z_c}{f_x}$$\n\n$$Y_c = (v-c_y)\\frac{Z_c}{f_y}$$\n\nNotations:\n- $f_x$: focal length in x direction\n- $f_y$: focal length in y direction\n- $c_x$: principal point in x direction (pixels)\n- $c_y$: principal point in y direction (pixels)\n- $(X_C, Y_C, Z_C)$: points in camera frame\n\nwhere $f_x$, $f_y$, $c_x$, $c_y$ specify the intrinsics of the camera.\n\nThe diagram of the pinhole camera model below is found from [OpenCV documentation](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html). Note that the $u$ and $v$ directions are reversed in Python due to the difference in the convention.","block_group":"9f909f1030714376a8126da88c09e343"},{"cell_type":"markdown","metadata":{"id":"OaXlVRcskbDf","cell_id":"9a0893a5e05b46f7ba9242c3540e6419","deepnote_cell_type":"markdown"},"source":"![](https://docs.opencv.org/3.4/pinhole_camera_model.png)","block_group":"955b19e235954f6e8bd3d23bd3329af0"},{"cell_type":"markdown","metadata":{"id":"5lOkEMMArZOo","cell_id":"8c411ea85df3423781f54128c623e6ee","deepnote_cell_type":"markdown"},"source":"## Generate Mask from Point Clouds\n\n**9.1a** Now given the points of the mustard bottle in the world frame p_W_mustard, can you re-project these points back to the image plane to construct the mask of the mustard bottle? Note that you may need to reverse u,v indices to get the mask of the mustard bottle upright. Your mask should be of the same size as the original depth image, which is (480, 640)","block_group":"0fe8b9867c664e5d9cb86da42795d54e"},{"cell_type":"code","metadata":{"id":"uSvk2b-WrZOp","cell_id":"8de7fe2a0e6c47be8da9a2a889f91cdb","deepnote_cell_type":"code"},"source":"def deproject_pW_to_image(p_W_mustard, cx, cy, fx, fy, X_WC):\n    \"\"\"\n    convert points in the world frame to camera pixels\n    Input:\n        - p_W_mustard: points of the mustard bottle in world frame (nx3)\n        - fx, fy, cx, cy: camera intrinsics\n        - X_WC: camera pose in the world frame\n    Output:\n        - mask: numpy array of size 480x640\n    \"\"\"\n\n    mask = np.zeros([480, 640])\n    return mask","block_group":"6a794592d60d4bc2ac382d20baec8d25","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"X8-LRTNMrZOt","outputId":"a09f2d5e-c051-445f-f118-6f0e635c89c3","scrolled":true,"cell_id":"ab3181da4a7f41a49b00fc62cc8d086b","deepnote_cell_type":"code"},"source":"cx, cy, fx, fy = env.get_intrinsics()\nmask = deproject_pW_to_image(p_W_mustard, cx, cy, fx, fy, X_WC)\nplt.imshow(mask)","block_group":"a79d7a4705604d1cb64defaaf59fd1bf","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"jAoTk3YfrZOy","cell_id":"24f8bcc10d8c4311b420f2aaca50ca58","deepnote_cell_type":"markdown"},"source":"You should be able to visually verify that the generated mask perfectly align with the mustard bottle!","block_group":"a8d8f16eba6541129c1fbe657eb69685"},{"cell_type":"code","metadata":{"id":"jlUvId9TrZOy","outputId":"4c486ab2-180f-4f23-b787-b409b04b97cf","cell_id":"602c4acbcbc943a88da13293d6836b45","deepnote_cell_type":"code"},"source":"plt.imshow(rgb_im)","block_group":"77e31b5f28524a1b9fca0017ce8e5bfc","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"PmMxCE6KrZO3","outputId":"95511567-dbd7-4b88-b0ca-65aa5c6d4b39","cell_id":"7f2eec3b1bed462cafb725443458b19e","deepnote_cell_type":"code"},"source":"masked_rgb = rgb_im * mask[:, :, np.newaxis].astype(int)\nplt.imshow(masked_rgb)","block_group":"c0cfcb4c05e2423f8ec0d69f6e593b9f","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"sJSzxkPzOrwc","cell_id":"8ec15ba13a354f0898eeec57f63164b4","deepnote_cell_type":"markdown"},"source":"# Analysis for Cluttered Scenes\n**Assume we have a scene with more than one object, i.e., we now have a Cheez-It box in the scene along with the mustard bottle. Let’s explore how our pipeline can work for generating object masks in these scenarios (suppose we can still easily crop the objects in the scene to a region about a flat empty surface). Answer the following questions.**\n\n**9.1.b** A direct and unmodified application of our pipeline above would use the full raw point cloud which includes both objects. Using this unmodified method, can you get the separate masks of the mustard bottle and Cheez-It box? Explain your reasoning.\n\nNow, Suppose we add an extra processing step in our pipeline, where we perform clustering on the point cloud and obtain labels for which points belong to each cluster (i.e., suppose we attempt to cluster it into two clusters using an algorithm like [DBSCAN](http://www.open3d.org/docs/latest/tutorial/Basic/pointcloud.html#DBSCAN-clustering))\n\n**9.1.c** Assume the Cheez-It box is relatively far away from the mustard bottle, i.e., they are not touching and there at least a few cm of free space in between them. With the objects separated, will our pipeline with clustering added be able to reliably provide separate masks of the mustard bottle and the Cheez-It box? Explain your reasoning.\n\n**9.1.d** Assume the Cheez-It box and mustard bottle are extremely close to each other, i.e., the mustard bottle might be resting on top of the flat Cheez-It box. With the objects touching each other, will our pipeline with clustering added be able to reliably provide separate masks of the mustard bottle and the Cheez-It box? Explain your reasoning.","block_group":"e0f3a7ff66cf40e088a49b915b3aff3f"},{"cell_type":"markdown","metadata":{"id":"kVAB7pkurZO7","cell_id":"cdfc00315004426d8275ed2a5fd1312c","deepnote_cell_type":"markdown"},"source":"# Generate Training Images and Masks via Data Augmentation","block_group":"df94ac52fa944524bcf946bc0a975696"},{"cell_type":"markdown","metadata":{"id":"nyiEhiNTrZO8","cell_id":"2e0199201995482a9699e316bab69213","deepnote_cell_type":"markdown"},"source":"A major benefit to leveraging models such as Mask-RCNN is that they have greater capability for handling complex scenes. As discussed before, a downside is that models tend to require large amounts of high quality training data in order to reach this level of robustness.\n\n[Data augmentation](https://en.wikipedia.org/wiki/Data_augmentation) is commonly used to generate more training data from the existing data. For example, a common trick to generate training images and masks for occluded scenes is to randomly insert rendered objects on top of the real image. Similarly, you can randomly scale, flip, rotate, duplicate, and crop to \"simulate\" more complex scenes.","block_group":"04f6558e861549cd96e6ccbacbcbff2e"},{"cell_type":"markdown","metadata":{"id":"AQ_P53yesblo","cell_id":"530e58165ae14a29bc356928806794f4","deepnote_cell_type":"markdown"},"source":"<figure>\n<center>\n<img src='https://developers.google.com/machine-learning/practica/image-classification/images/data_augmentation.png' />\n<figcaption>Example Image Data Augmentation (credit: developers.google.com)</figcaption></center>\n</figure>","block_group":"0447397a22e24faa8ff69c39bee72919"},{"cell_type":"markdown","metadata":{"id":"Zs3arsiQsY_a","cell_id":"791385ea8c4040aea878cfabede33550","deepnote_cell_type":"markdown"},"source":"In this exercise, we ask you to explore different ways to augment from our existing mustard bottle image:\n- flipping\n- translating\n- duplication \n- cropping\n- adding noise \n\n**9.1e** **Please complete the function below to generate 1 more pair of a color image and mask label using at least 2 tricks above to augment your data. You may use Numpy only!** \n\n**Note: make sure you display both of the your new image and mask below in your notebook submission. Also submit your new image and mask in your written submission. Your results will be visually graded**","block_group":"97d158e3fdad4c7c8ab243e5521f98ca"},{"cell_type":"code","metadata":{"id":"UTziZ7y5rZO9","cell_id":"66a62e2932ee4c2dbd6fa22ee2b2d8b0","deepnote_cell_type":"code"},"source":"def augment_mustard_bottle(rgb_im, mask):\n    \"\"\"\n    perform random rotation, scaling, and duplication to generate\n    more training images and labels\n    rgb_im: original rgb image of the mustard bottle\n    mask: binary mask of the mustard bottle\n    \"\"\"\n    augmented_rgb = np.zeros((480, 640, 3))\n    augmented_mask = np.zeros((480, 640))\n    return augmented_rgb, augmented_mask","block_group":"54623f736c214ddc9364d068bd1cb9e2","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"e8a1012f1cbd4e09a9680da1f25a6567","deepnote_cell_type":"code"},"source":"# Translation with flip over x-axis\n\n\ndef augment_mustard_bottle(rgb_im, mask):\n    \"\"\"\n    perform random rotation, scaling, and duplication to generate\n    more training images and labels\n    rgb_im: original rgb image of the mustard bottle\n    mask: binay mask of the mustard bottle\n    \"\"\"\n    augmented_rgb = np.zeros((480, 640, 3))\n    augmented_mask = np.zeros((480, 640))\n\n    augmented_rgb = np.roll(np.flip(rgb_im, axis=0), 200, axis=1)\n    augmented_mask = np.roll(np.flip(mask, axis=0), 200, axis=1)\n\n    return augmented_rgb, augmented_mask","block_group":"fae76de67bb5438089f66e55ee348f72","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"3f_Z3aWErZPE","cell_id":"33cddf34a5534139be34dac4e27b352a","deepnote_cell_type":"code"},"source":"new_img, new_mask = augment_mustard_bottle(rgb_im, mask)","block_group":"8c6512c084b6443e9d3e1d0394321b57","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"h18zQE06rZPI","outputId":"998c4c48-044d-43df-e615-5600d2514bd6","cell_id":"2d9e80e182e7422d848ed1754e3a9206","deepnote_cell_type":"code"},"source":"plt.imshow(new_img)","block_group":"89c1619dc3d44c4d9a63a782cbc751a5","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"jcbFVSugrZPM","outputId":"e05d2d9d-def0-4f7a-863e-64cdcba8074e","cell_id":"c7013dcb15f14bf08702c1c8ca3b3c1f","deepnote_cell_type":"code"},"source":"plt.imshow(new_mask)","block_group":"2b3e53056cfd4cc7b688790951292453","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"MwE8yNg58VQN","cell_id":"b12d8dba2c2942e39b3d22d563f7e9e6","deepnote_cell_type":"markdown"},"source":"## How will this notebook be Graded?\n\nIf you are enrolled in the class, this notebook will be graded using [Gradescope](www.gradescope.com). You should have gotten the enrollement code on our announcement in Piazza. \n\nFor submission of this assignment, you must do two things. \n- Download and submit the notebook `label_generation.ipynb` to Gradescope's notebook submission section, along with your notebook for the other problems.\n- Write down your answers to 9.1.b, 9.1.c, and 9.1.d, and add your images from 9.1.e to a separate pdf file and submit it to Gradescope's written submission section. \n\nWe will evaluate the local functions in the notebook to see if the function behaves as we have expected. For this exercise, the rubric is as follows:\n- [3 pts] Correct Implementation of `deproject_pW_to_image` method.\n- [3 pts] Analysis for Cluttered Scenes: reasonable answers and explanations. \n- [2 pts] Visually reasonable output from `augment_mustard_bottle`.\n","block_group":"183f30ae61294daa89d74191aa1a7fae"},{"cell_type":"code","metadata":{"id":"mKlkgkjjXIKP","colab":{"height":381,"base_uri":"https://localhost:8080/"},"outputId":"c1932c14-be4f-4b4e-fe37-42984518829a","cell_id":"44fa743afca14e33b6d798c8c1d56895","deepnote_cell_type":"code"},"source":"from manipulation.exercises.grader import Grader\nfrom manipulation.exercises.segmentation.test_mask import TestMask\n\nGrader.grade_output([TestMask], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")","block_group":"e3b17fdcf2c4463eb8af642c7502fd71","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=deee6dca-8c5f-4baa-bc36-a4211e6ce8ad' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"18bcf231b0984e8bb71e0feae2105d4d","deepnote_execution_queue":[]}}